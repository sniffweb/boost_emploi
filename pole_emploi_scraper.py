"""
Module de scraping pour les offres d'emploi de P√¥le Emploi
"""

import requests
from bs4 import BeautifulSoup
import time
import re
from urllib.parse import urljoin, urlparse, parse_qs
from config import (
    POLE_EMPLOI_SEARCH_URL, 
    DEFAULT_HEADERS, 
    REQUEST_DELAY,
    COLORS
)


class PoleEmploiScraper:
    """Classe pour scraper les offres d'emploi sur P√¥le Emploi"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(DEFAULT_HEADERS)
        self.base_url = "https://candidat.pole-emploi.fr"
        
    def search_jobs(self, keywords="", location="", contract_type="TOUS", max_results=50):
        """
        Recherche des offres d'emploi sur P√¥le Emploi
        
        Args:
            keywords (str): Mots-cl√©s de recherche
            location (str): Localisation (ville, d√©partement, r√©gion)
            contract_type (str): Type de contrat
            max_results (int): Nombre maximum de r√©sultats √† retourner
            
        Returns:
            list: Liste des offres d'emploi trouv√©es
        """
        print(f"{COLORS['INFO']}üîç Recherche d'offres d'emploi en cours...{COLORS['END']}")
        print(f"   Mots-cl√©s: {keywords}")
        print(f"   Localisation: {location}")
        print(f"   Type de contrat: {contract_type}")
        
        jobs = []
        page = 1
        max_pages = 10  # Limiter le nombre de pages √† scraper
        
        try:
            while len(jobs) < max_results and page <= max_pages:
                print(f"{COLORS['INFO']}   üìÑ Scraping page {page}...{COLORS['END']}")
                
                # Construire les param√®tres de recherche
                params = self._build_search_params(keywords, location, contract_type, page)
                
                # Effectuer la requ√™te
                response = self._make_request(POLE_EMPLOI_SEARCH_URL, params)
                
                if not response:
                    break
                    
                # Parser les r√©sultats
                page_jobs = self._parse_job_listings(response.text)
                
                if not page_jobs:
                    print(f"{COLORS['WARNING']}   ‚ö†Ô∏è Aucune offre trouv√©e sur la page {page}{COLORS['END']}")
                    break
                    
                jobs.extend(page_jobs)
                print(f"{COLORS['SUCCESS']}   ‚úÖ {len(page_jobs)} offres trouv√©es sur la page {page}{COLORS['END']}")
                
                page += 1
                time.sleep(REQUEST_DELAY)
                
            # Limiter les r√©sultats au maximum demand√©
            jobs = jobs[:max_results]
            
            print(f"{COLORS['SUCCESS']}üéâ Recherche termin√©e: {len(jobs)} offres trouv√©es{COLORS['END']}")
            
        except Exception as e:
            print(f"{COLORS['ERROR']}‚ùå Erreur lors de la recherche: {str(e)}{COLORS['END']}")
            
        return jobs
    
    def _build_search_params(self, keywords, location, contract_type, page):
        """Construit les param√®tres de recherche pour l'URL"""
        params = {
            'motsCles': keywords,
            'lieux': location,
            'page': str(page),
            'offresPartenaires': 'true',
            'rayon': '10',
            'tri': '0',
            'minCreationDate': '',
            'maxCreationDate': '',
            'typeContrat': contract_type if contract_type != 'TOUS' else '',
            'natureContrat': '',
            'salaireMin': '',
            'salaireMax': '',
            'experience': '',
            'qualification': '',
            'secteurActivite': '',
            'typeConvention': '',
            'nomEntreprise': '',
            'lieuTravail': '',
            'filtresRecherche': '',
            'origineOffre': 'PE',
            'origineOffre': 'PARTENAIRES'
        }
        
        # Nettoyer les param√®tres vides
        return {k: v for k, v in params.items() if v}
    
    def _make_request(self, url, params=None):
        """Effectue une requ√™te HTTP avec gestion d'erreurs"""
        try:
            response = self.session.get(url, params=params, timeout=30)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            print(f"{COLORS['ERROR']}‚ùå Erreur de requ√™te: {str(e)}{COLORS['END']}")
            return None
    
    def _parse_job_listings(self, html_content):
        """Parse le contenu HTML pour extraire les offres d'emploi"""
        soup = BeautifulSoup(html_content, 'html.parser')
        jobs = []
        
        # S√©lecteurs pour les offres d'emploi sur P√¥le Emploi
        job_selectors = [
            'article[data-testid="offre-emploi"]',
            '.titreMedia',
            '.result',
            '.job-result',
            '[data-testid="job-offer"]'
        ]
        
        job_elements = []
        for selector in job_selectors:
            job_elements = soup.select(selector)
            if job_elements:
                break
        
        if not job_elements:
            # Essayer de trouver les offres avec d'autres s√©lecteurs
            job_elements = soup.find_all(['article', 'div'], class_=re.compile(r'(offre|job|result|emploi)'))
        
        for element in job_elements:
            job_data = self._extract_job_data(element)
            if job_data:
                jobs.append(job_data)
        
        return jobs
    
    def _extract_job_data(self, element):
        """Extrait les donn√©es d'une offre d'emploi"""
        try:
            # Titre de l'offre
            title_selectors = [
                'h3 a', 'h2 a', '.titreMedia a', 'a[data-testid="offre-lien"]',
                'a.titreOffre', '.title a', 'h3', 'h2'
            ]
            
            title_element = None
            title = "Titre non trouv√©"
            job_url = ""
            
            for selector in title_selectors:
                title_element = element.select_one(selector)
                if title_element:
                    title = title_element.get_text(strip=True)
                    # Essayer d'extraire l'URL
                    if title_element.name == 'a':
                        href = title_element.get('href', '')
                        if href:
                            job_url = urljoin(self.base_url, href)
                    else:
                        # Chercher un lien √† proximit√©
                        link = element.find('a', href=True)
                        if link:
                            job_url = urljoin(self.base_url, link['href'])
                    break
            
            # Entreprise
            company_selectors = [
                '.entreprise', '.company', '.employeur', '.societe',
                '[data-testid="entreprise"]', '.nomEntreprise'
            ]
            
            company = "Entreprise non sp√©cifi√©e"
            for selector in company_selectors:
                company_element = element.select_one(selector)
                if company_element:
                    company = company_element.get_text(strip=True)
                    break
            
            # Localisation
            location_selectors = [
                '.lieu', '.location', '.ville', '.adresse',
                '[data-testid="lieu"]', '.localisation'
            ]
            
            location = "Localisation non sp√©cifi√©e"
            for selector in location_selectors:
                location_element = element.select_one(selector)
                if location_element:
                    location = location_element.get_text(strip=True)
                    break
            
            # Type de contrat
            contract_selectors = [
                '.typeContrat', '.contract', '.contrat', '.type',
                '[data-testid="type-contrat"]', '.natureContrat'
            ]
            
            contract_type = "Type non sp√©cifi√©"
            for selector in contract_selectors:
                contract_element = element.select_one(selector)
                if contract_element:
                    contract_type = contract_element.get_text(strip=True)
                    break
            
            # Date de publication
            date_selectors = [
                '.date', '.publication', '.creation', '.datePublication',
                '[data-testid="date"]', '.dateOffre'
            ]
            
            date = "Date non sp√©cifi√©e"
            for selector in date_selectors:
                date_element = element.select_one(selector)
                if date_element:
                    date = date_element.get_text(strip=True)
                    break
            
            # Description (tronqu√©e)
            description_selectors = [
                '.description', '.resume', '.extrait', '.summary',
                '[data-testid="description"]', '.texteOffre'
            ]
            
            description = ""
            for selector in description_selectors:
                desc_element = element.select_one(selector)
                if desc_element:
                    description = desc_element.get_text(strip=True)[:200] + "..."
                    break
            
            return {
                'title': title,
                'company': company,
                'location': location,
                'contract_type': contract_type,
                'date': date,
                'description': description,
                'url': job_url,
                'source': 'P√¥le Emploi'
            }
            
        except Exception as e:
            print(f"{COLORS['WARNING']}‚ö†Ô∏è Erreur lors de l'extraction des donn√©es: {str(e)}{COLORS['END']}")
            return None
    
    def get_job_details(self, job_url):
        """
        R√©cup√®re les d√©tails complets d'une offre d'emploi
        
        Args:
            job_url (str): URL de l'offre d'emploi
            
        Returns:
            dict: D√©tails complets de l'offre
        """
        try:
            response = self._make_request(job_url)
            if not response:
                return None
                
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Extraire les d√©tails complets
            details = {
                'full_description': '',
                'requirements': '',
                'benefits': '',
                'salary': '',
                'contact_info': ''
            }
            
            # Description compl√®te
            desc_selectors = [
                '.descriptionOffre', '.description', '.contenuOffre',
                '[data-testid="description-complete"]', '.texteComplet'
            ]
            
            for selector in desc_selectors:
                desc_element = soup.select_one(selector)
                if desc_element:
                    details['full_description'] = desc_element.get_text(strip=True)
                    break
            
            # Salaire
            salary_selectors = [
                '.salaire', '.remuneration', '.salary', '.salaireOffre'
            ]
            
            for selector in salary_selectors:
                salary_element = soup.select_one(selector)
                if salary_element:
                    details['salary'] = salary_element.get_text(strip=True)
                    break
            
            return details
            
        except Exception as e:
            print(f"{COLORS['ERROR']}‚ùå Erreur lors de la r√©cup√©ration des d√©tails: {str(e)}{COLORS['END']}")
            return None
    
    def filter_jobs(self, jobs, filters):
        """
        Filtre les offres d'emploi selon les crit√®res sp√©cifi√©s
        
        Args:
            jobs (list): Liste des offres d'emploi
            filters (dict): Crit√®res de filtrage
            
        Returns:
            list: Offres filtr√©es
        """
        filtered_jobs = jobs.copy()
        
        # Filtrage par mots-cl√©s
        if filters.get('keywords'):
            keywords = filters['keywords'].lower()
            filtered_jobs = [
                job for job in filtered_jobs
                if keywords in job['title'].lower() or 
                   keywords in job['description'].lower() or
                   keywords in job['company'].lower()
            ]
        
        # Filtrage par type de contrat
        if filters.get('contract_type') and filters['contract_type'] != 'TOUS':
            contract = filters['contract_type'].lower()
            filtered_jobs = [
                job for job in filtered_jobs
                if contract in job['contract_type'].lower()
            ]
        
        # Filtrage par localisation
        if filters.get('location'):
            location = filters['location'].lower()
            filtered_jobs = [
                job for job in filtered_jobs
                if location in job['location'].lower()
            ]
        
        return filtered_jobs
